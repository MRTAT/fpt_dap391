import streamlit as st
import os
import google.generativeai as genai
import torchvision.transforms as transforms
import torch
from PIL import Image
from model_lab1 import Cnn


def setup_gemini():
    genai.configure(api_key='AIzaSyDyG1I9Z0ZaID1EsSd9yBeaqxnWGFOC7sI')
    try:
        models = genai.list_models()  # Kiá»ƒm tra mÃ´ hÃ¬nh cÃ³ sáºµn
        print("Available models:", models)
        model = genai.GenerativeModel('gemini-1.5-pro-latest')
        return model
    except Exception as e:
        print(f"Error listing models: {e}")
        return None

# Disease prediction using Gemini
def get_gemini_response(model, symptoms, lang="en"):
    if lang == "en":
        prompt = f"""
        Based on the following symptoms, suggest possible medical conditions and provide brief explanations:
        Symptoms: {symptoms}

        Please format your response as follows:
        1. Most likely conditions
        2. Brief explanation for each condition
        3. General recommendations
        4. Treatment details and the name of the medication that should be used

        Note: This is not a medical diagnosis. Please consult a healthcare professional for proper medical advice.
        """
    else:  # Vietnamese
        prompt = f"""
        Dá»±a trÃªn cÃ¡c triá»‡u chá»©ng sau Ä‘Ã¢y, hÃ£y Ä‘á» xuáº¥t cÃ¡c tÃ¬nh tráº¡ng y táº¿ cÃ³ thá»ƒ xáº£y ra vÃ  cung cáº¥p giáº£i thÃ­ch ngáº¯n gá»n:
        Triá»‡u chá»©ng: {symptoms}

        Vui lÃ²ng Ä‘á»‹nh dáº¡ng pháº£n há»“i cá»§a báº¡n nhÆ° sau:
        1. CÃ¡c tÃ¬nh tráº¡ng cÃ³ kháº£ nÄƒng xáº£y ra nháº¥t
        2. Giáº£i thÃ­ch ngáº¯n gá»n cho tá»«ng tÃ¬nh tráº¡ng
        3. CÃ¡c khuyáº¿n nghá»‹ chung
        4. Chi tiáº¿t vá» cÃ¡ch Ä‘iá»u trá»‹ vÃ  cÃ¡c tÃªn cá»§a loáº¡i thuá»‘c nÃªn sá»­ dá»¥ng

        LÆ°u Ã½: ÄÃ¢y khÃ´ng pháº£i lÃ  cháº©n Ä‘oÃ¡n y táº¿. Vui lÃ²ng tham kháº£o Ã½ kiáº¿n cá»§a chuyÃªn gia y táº¿ Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n y táº¿ thÃ­ch há»£p.
        """

    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Error in generating response: {str(e)}"


def preprocess_image(image_path):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),  # Resize vá» kÃ­ch thÆ°á»›c phÃ¹ há»£p
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    return transform(image_path).unsqueeze(0)  # ThÃªm chiá»u cho áº£nh


def load_model():
    # Sá»­ dá»¥ng thiáº¿t bá»‹ phÃ¹ há»£p (CPU hoáº·c GPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Khá»Ÿi táº¡o mÃ´ hÃ¬nh Cnn vá»›i 4 lá»›p phÃ¢n loáº¡i
    model = Cnn(num_classes=4).to(device)

    # ÄÆ°á»ng dáº«n Ä‘áº¿n file checkpoint
    checkpoint_path = os.path.join("trained_model", "best_cnn.pt")

    if not os.path.exists(checkpoint_path):
        st.error(f"KhÃ´ng tÃ¬m tháº¥y file model táº¡i {checkpoint_path}")
        return None

    try:
        # CÃ¡ch 1: Load vá»›i weights_only=False (cÃ¡ch an toÃ n nháº¥t náº¿u báº¡n tin tÆ°á»Ÿng nguá»“n checkpoint)
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint["model"])
    except Exception as e:
        try:
            # CÃ¡ch 2: Thá»­ thÃªm numpy.core.multiarray.scalar vÃ o danh sÃ¡ch safe globals
            import numpy as np
            from torch.serialization import add_safe_globals

            # ThÃªm numpy.core.multiarray.scalar vÃ o danh sÃ¡ch safe globals
            add_safe_globals([np.core.multiarray.scalar])

            # Thá»­ load láº¡i vá»›i weights_only=True (máº·c Ä‘á»‹nh)
            checkpoint = torch.load(checkpoint_path, map_location=device)
            model.load_state_dict(checkpoint["model"])
        except Exception as inner_e:
            st.error(f"KhÃ´ng thá»ƒ load model: {str(inner_e)}")
            return None

    # Chuyá»ƒn sang cháº¿ Ä‘á»™ dá»± Ä‘oÃ¡n
    model.eval()

    return model


# Streamlit page config
st.set_page_config(page_title="Chatbot AI", page_icon="ğŸ¤–", layout="wide")
st.markdown(
    """
    <style>
        .stChatMessage { border-radius: 10px; padding: 10px; margin: 5px 0; }
        .user { background-color: #dcf8c6; text-align: right; }
        .bot { background-color: #f1f1f1; }

        /* CSS Ä‘á»ƒ tá»± Ä‘á»™ng cuá»™n xuá»‘ng pháº§n chat */
        #chat-container {
            max-height: 600px;
            overflow-y: auto;
        }

        /* XÃ³a padding dÆ°á»›i cho container */
        .block-container {
            padding-bottom: 2rem;
        }

        /* Make columns equal height */
        .main-columns {
            display: flex;
            min-height: 700px;
        }

        /* Add some space between columns */
        .column-gap {
            padding: 0 10px;
        }

        /* Language toggle styling */
        .language-toggle {
            display: flex;
            justify-content: center;
            align-items: center;
            margin-bottom: 10px;
        }

        .language-button {
            margin: 0 5px;
            padding: 5px 10px;
            border-radius: 5px;
            cursor: pointer;
            font-weight: bold;
        }

        .active-lang {
            background-color: #4CAF50;
            color: white;
        }

        .inactive-lang {
            background-color: #f1f1f1;
            color: black;
        }
    </style>

    <script>
        // JavaScript Ä‘á»ƒ tá»± Ä‘á»™ng cuá»™n xuá»‘ng cuá»‘i pháº§n chat
        function scrollToBottom() {
            var chatContainer = document.getElementById('chat-container');
            if (chatContainer) {
                chatContainer.scrollTop = chatContainer.scrollHeight;
            }
        }
        // Cháº¡y hÃ m khi trang Ä‘Ã£ táº£i xong
        window.addEventListener('load', scrollToBottom);
    </script>
    """,
    unsafe_allow_html=True,
)

# Initialize language selection if not already set
if "language" not in st.session_state:
    st.session_state.language = "en"

# Check URL parameters for language change - PUT THIS BEFORE THE BUTTONS
query_params = st.query_params
if "lang" in query_params:
    if query_params["lang"] == "vn" and st.session_state.language != "vn":
        st.session_state.language = "vn"
        st.rerun()
    elif query_params["lang"] == "en" and st.session_state.language != "en":
        st.session_state.language = "en"
        st.rerun()


# Function to get text based on current language
def get_text(en_text, vn_text):
    return en_text if st.session_state.language == "en" else vn_text


# Call GPT
gemini_model = setup_gemini()

# Language toggle with buttons instead of HTML
col_spacer, lang_col = st.columns([10, 2])  # Táº¡o khÃ´ng gian rá»™ng bÃªn trÃ¡i, chá»‰ sá»­ dá»¥ng 2/12 khÃ´ng gian cho nÃºt ngÃ´n ngá»¯

with lang_col:
    # Táº¡o 2 cá»™t bÃªn trong cá»™t ngÃ´n ngá»¯ Ä‘á»ƒ Ä‘áº·t cÃ¡c nÃºt gáº§n nhau
    en_col, vn_col = st.columns(2)
    with en_col:
        if st.button("ğŸ‡¬ğŸ‡§ EN", type="primary" if st.session_state.language == "en" else "secondary"):
            st.session_state.language = "en"
            st.query_params["lang"] = "en"
            st.rerun()
    with vn_col:
        if st.button("ğŸ‡»ğŸ‡³ VN", type="primary" if st.session_state.language == "vn" else "secondary"):
            st.session_state.language = "vn"
            st.query_params["lang"] = "vn"
            st.rerun()

# TiÃªu Ä‘á» chÃ­nh cho trang
st.markdown(
    f"<h1 style='text-align: center;'>ğŸ’¬ {get_text('Chatbot AI - Healthcare services', 'Chatbot AI - Dá»‹ch vá»¥ chÄƒm sÃ³c sá»©c khá»e')} ğŸ‡»ğŸ‡³</h1>",
    unsafe_allow_html=True
)

# Create two main columns for the page layout
left_col, right_col = st.columns(2)

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []


# Function to get treatment recommendations based on current language
def get_treatment_recommendations(model, condition, lang="en"):
    if lang == "en":
        prompt = f"""
        Provide evidence-based treatment recommendations and care guidelines for a patient with {condition} diagnosis from an X-ray.

        Please format your response as follows:
        1. Brief explanation of the condition
        2. Standard treatment approaches
        3. Home care recommendations
        4. When to see a doctor
        5. Treatment details and the name of the medication that should be used


        Note: This is for informational purposes. Always consult healthcare professionals for proper medical advice.
        """
    else:  # Vietnamese
        prompt = f"""
        Cung cáº¥p cÃ¡c khuyáº¿n nghá»‹ Ä‘iá»u trá»‹ dá»±a trÃªn báº±ng chá»©ng vÃ  hÆ°á»›ng dáº«n chÄƒm sÃ³c cho bá»‡nh nhÃ¢n cÃ³ cháº©n Ä‘oÃ¡n {condition} tá»« hÃ¬nh áº£nh X-quang.

        Vui lÃ²ng Ä‘á»‹nh dáº¡ng pháº£n há»“i cá»§a báº¡n nhÆ° sau:
        1. Giáº£i thÃ­ch ngáº¯n gá»n vá» tÃ¬nh tráº¡ng
        2. PhÆ°Æ¡ng phÃ¡p Ä‘iá»u trá»‹ tiÃªu chuáº©n
        3. Khuyáº¿n nghá»‹ chÄƒm sÃ³c táº¡i nhÃ 
        4. Khi nÃ o nÃªn gáº·p bÃ¡c sÄ©
        5. Chi tiáº¿t vá» cÃ¡ch Ä‘iá»u trá»‹ vÃ  cÃ¡c tÃªn cá»§a loáº¡i thuá»‘c nÃªn sá»­ dá»¥ng
        
        LÆ°u Ã½: ÄÃ¢y lÃ  thÃ´ng tin tham kháº£o. LuÃ´n tham kháº£o Ã½ kiáº¿n cá»§a chuyÃªn gia y táº¿ Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n y táº¿ thÃ­ch há»£p.
        """

    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        error_msg = "Error in generating treatment recommendations" if lang == "en" else "Lá»—i khi táº¡o khuyáº¿n nghá»‹ Ä‘iá»u trá»‹"
        return f"{error_msg}: {str(e)}"


# RIGHT COLUMN - X-ray Prediction
with right_col:
    st.subheader(get_text("ğŸ“¸ X-ray Prediction", "ğŸ“¸ Dá»± Ä‘oÃ¡n X-quang"))

    uploaded_file = st.file_uploader(
        get_text("Choose X-ray (JPG/PNG)", "Chá»n áº£nh X-quang (JPG/PNG)"),
        type=["jpg", "png"]
    )

    if uploaded_file is not None:
        st.success(get_text("âœ… Image uploaded successfully!", "âœ… Táº£i áº£nh lÃªn thÃ nh cÃ´ng!"))

        # Add a toggle to show/hide the image
        show_image = st.checkbox(get_text("Show Image", "Hiá»ƒn thá»‹ áº£nh"), value=True)

        # Only display the image if the toggle is on
        if show_image:
            st.image(uploaded_file, caption=" ", use_container_width=True)

        predict_button = st.button(get_text("Predict", "Dá»± Ä‘oÃ¡n"))

        if predict_button:
            try:
                # Image preprocessing
                image = Image.open(uploaded_file).convert("RGB")

                # Láº¥y thiáº¿t bá»‹ hiá»‡n táº¡i
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

                # Dá»± Ä‘oÃ¡n káº¿t quáº£
                model_lung = load_model()

                if model_lung is not None:
                    tensor = preprocess_image(image).to(device)
                    with torch.no_grad():
                        output = model_lung(tensor)
                        prediction = torch.argmax(output, 1).item()

                        classes = ['Covid', 'Normal', 'Pneumonia', 'Tuberculosis']
                        condition = classes[prediction]

                        result_prefix = "ğŸ§‘ğŸ½â€âš•ï¸ ğŸ™ğŸ½ **Prediction:**" if st.session_state.language == "en" else "ğŸ§‘ğŸ½â€âš•ï¸ ğŸ™ğŸ½ **Káº¿t quáº£ dá»± Ä‘oÃ¡n:**"
                        result = f"{result_prefix} {condition}"
                        st.session_state.prediction_result = result
                        st.success(result)

                        # Get treatment recommendations for the condition
                        if condition != "Normal":
                            st.info(
                                get_text("Generating treatment recommendations...", "Äang táº¡o khuyáº¿n nghá»‹ Ä‘iá»u trá»‹..."))
                            treatment_info = get_treatment_recommendations(gemini_model, condition,
                                                                           st.session_state.language)
                            st.markdown(get_text("### Treatment Recommendations", "### Khuyáº¿n nghá»‹ Ä‘iá»u trá»‹"))
                            st.markdown(treatment_info)
                        else:
                            st.markdown(get_text("### Normal X-ray", "### X-quang bÃ¬nh thÆ°á»ng"))
                            normal_text = get_text(
                                "No specific treatment needed as the X-ray appears normal. Continue with regular health practices and consult with a healthcare provider for any persistent symptoms.",
                                "KhÃ´ng cáº§n Ä‘iá»u trá»‹ cá»¥ thá»ƒ vÃ¬ X-quang cÃ³ váº» bÃ¬nh thÆ°á»ng. Tiáº¿p tá»¥c thá»±c hÃ nh sá»©c khá»e thÃ´ng thÆ°á»ng vÃ  tham kháº£o Ã½ kiáº¿n cá»§a nhÃ  cung cáº¥p dá»‹ch vá»¥ chÄƒm sÃ³c sá»©c khá»e náº¿u cÃ³ báº¥t ká»³ triá»‡u chá»©ng dai dáº³ng nÃ o."
                            )
                            st.markdown(normal_text)

                else:
                    error_msg = "Cannot load model. Please, check your path." if st.session_state.language == "en" else "KhÃ´ng thá»ƒ táº£i mÃ´ hÃ¬nh. Vui lÃ²ng kiá»ƒm tra Ä‘Æ°á»ng dáº«n cá»§a báº¡n."
                    st.error(error_msg)
            except Exception as e:
                error_prefix = "Error:" if st.session_state.language == "en" else "Lá»—i:"
                st.error(f"{error_prefix} {str(e)}")

# LEFT COLUMN - Chat Interface
with left_col:
    st.subheader(get_text("Chat with AI ğŸ–¥ï¸", "TrÃ² chuyá»‡n vá»›i AI ğŸ–¥ï¸"))

    # Display chat history with ID for JavaScript
    st.markdown('<div id="chat-container">', unsafe_allow_html=True)
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
    st.markdown('</div>', unsafe_allow_html=True)

    # Form Ä‘á»ƒ trÃ¡nh auto-rerun khi nháº­p liá»‡u
    with st.form(key="chat_form", clear_on_submit=True):
        placeholder_text = get_text(
            "Clearly state your symptoms here, please ğŸ€",
            "Vui lÃ²ng mÃ´ táº£ rÃµ rÃ ng cÃ¡c triá»‡u chá»©ng cá»§a báº¡n á»Ÿ Ä‘Ã¢y ğŸ€"
        )
        input_text = st.text_input("", placeholder=placeholder_text, key="user_input")
        submit_button = st.form_submit_button(get_text("Send", "Gá»­i"))

    # Xá»­ lÃ½ khi cÃ³ tin nháº¯n má»›i
    if submit_button and input_text.strip():
        # ThÃªm tin nháº¯n ngÆ°á»i dÃ¹ng vÃ o session state
        user_prefix = "You:" if st.session_state.language == "en" else "Báº¡n:"
        st.session_state.messages.append({"role": "user", "content": f"**{user_prefix}** {input_text}"})

        # Generate AI response
        thinking_text = "ğŸ¤– Thinking..." if st.session_state.language == "en" else "ğŸ¤– Äang suy nghÄ©..."
        with st.spinner(thinking_text):
            response = get_gemini_response(gemini_model, input_text, st.session_state.language)

        # ThÃªm pháº£n há»“i cá»§a AI vÃ o session state
        ai_prefix = "AI:" if st.session_state.language == "en" else "AI:"
        st.session_state.messages.append({"role": "assistant", "content": f"**{ai_prefix}** {response}"})

        # LÃ m má»›i trang má»™t láº§n Ä‘á»ƒ hiá»ƒn thá»‹ tin nháº¯n má»›i
        st.rerun()
