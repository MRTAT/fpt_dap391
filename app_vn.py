import streamlit as st
import os
import google.generativeai as genai
import torchvision.transforms as transforms
import torch
from PIL import Image
from model_lab1 import Cnn


def setup_gemini():
    genai.configure(api_key='AIzaSyDyG1I9Z0ZaID1EsSd9yBeaqxnWGFOC7sI')
    try:
        models = genai.list_models()  # Ki·ªÉm tra m√¥ h√¨nh c√≥ s·∫µn
        print("Available models:", models)
        model = genai.GenerativeModel('gemini-1.5-pro-latest')
        return model
    except Exception as e:
        print(f"Error listing models: {e}")
        return None

# Disease prediction using Gemini
def get_gemini_response(model, symptoms, lang="en"):
    if lang == "en":
        prompt = f"""
        Based on the following symptoms, suggest possible medical conditions and provide brief explanations:
        Symptoms: {symptoms}

        Please format your response as follows:
        1. Most likely conditions
        2. Brief explanation for each condition
        3. General recommendations
        4. Treatment details and the name of the medication that should be used

        Note: This is not a medical diagnosis. Please consult a healthcare professional for proper medical advice.
        """
    else:  # Vietnamese
        prompt = f"""
        D·ª±a tr√™n c√°c tri·ªáu ch·ª©ng sau ƒë√¢y, h√£y ƒë·ªÅ xu·∫•t c√°c t√¨nh tr·∫°ng y t·∫ø c√≥ th·ªÉ x·∫£y ra v√† cung c·∫•p gi·∫£i th√≠ch ng·∫Øn g·ªçn:
        Tri·ªáu ch·ª©ng: {symptoms}

        Vui l√≤ng ƒë·ªãnh d·∫°ng ph·∫£n h·ªìi c·ªßa b·∫°n nh∆∞ sau:
        1. C√°c t√¨nh tr·∫°ng c√≥ kh·∫£ nƒÉng x·∫£y ra nh·∫•t
        2. Gi·∫£i th√≠ch ng·∫Øn g·ªçn cho t·ª´ng t√¨nh tr·∫°ng
        3. C√°c khuy·∫øn ngh·ªã chung
        4. Chi ti·∫øt v·ªÅ c√°ch ƒëi·ªÅu tr·ªã v√† c√°c t√™n c·ªßa lo·∫°i thu·ªëc n√™n s·ª≠ d·ª•ng

        L∆∞u √Ω: ƒê√¢y kh√¥ng ph·∫£i l√† ch·∫©n ƒëo√°n y t·∫ø. Vui l√≤ng tham kh·∫£o √Ω ki·∫øn c·ªßa chuy√™n gia y t·∫ø ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n y t·∫ø th√≠ch h·ª£p.
        """

    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Error in generating response: {str(e)}"


def preprocess_image(image_path):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),  # Resize v·ªÅ k√≠ch th∆∞·ªõc ph√π h·ª£p
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    return transform(image_path).unsqueeze(0)  # Th√™m chi·ªÅu cho ·∫£nh


def load_model():
    # S·ª≠ d·ª•ng thi·∫øt b·ªã ph√π h·ª£p (CPU ho·∫∑c GPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Kh·ªüi t·∫°o m√¥ h√¨nh Cnn v·ªõi 4 l·ªõp ph√¢n lo·∫°i
    model = Cnn(num_classes=4).to(device)

    # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file checkpoint
    checkpoint_path = os.path.join("trained_model", "best_cnn.pt")

    if not os.path.exists(checkpoint_path):
        st.error(f"Kh√¥ng t√¨m th·∫•y file model t·∫°i {checkpoint_path}")
        return None

    try:
        # C√°ch 1: Load v·ªõi weights_only=False (c√°ch an to√†n nh·∫•t n·∫øu b·∫°n tin t∆∞·ªüng ngu·ªìn checkpoint)
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint["model"])
    except Exception as e:
        try:
            # C√°ch 2: Th·ª≠ th√™m numpy.core.multiarray.scalar v√†o danh s√°ch safe globals
            import numpy as np
            from torch.serialization import add_safe_globals

            # Th√™m numpy.core.multiarray.scalar v√†o danh s√°ch safe globals
            add_safe_globals([np.core.multiarray.scalar])

            # Th·ª≠ load l·∫°i v·ªõi weights_only=True (m·∫∑c ƒë·ªãnh)
            checkpoint = torch.load(checkpoint_path, map_location=device)
            model.load_state_dict(checkpoint["model"])
        except Exception as inner_e:
            st.error(f"Kh√¥ng th·ªÉ load model: {str(inner_e)}")
            return None

    # Chuy·ªÉn sang ch·∫ø ƒë·ªô d·ª± ƒëo√°n
    model.eval()

    return model


# Streamlit page config
st.set_page_config(page_title="Chatbot AI", page_icon="ü§ñ", layout="wide")
st.markdown(
    """
    <style>
        .stChatMessage { border-radius: 10px; padding: 10px; margin: 5px 0; }
        .user { background-color: #dcf8c6; text-align: right; }
        .bot { background-color: #f1f1f1; }

        /* CSS ƒë·ªÉ t·ª± ƒë·ªông cu·ªôn xu·ªëng ph·∫ßn chat */
        #chat-container {
            max-height: 600px;
            overflow-y: auto;
        }

        /* X√≥a padding d∆∞·ªõi cho container */
        .block-container {
            padding-bottom: 2rem;
        }

        /* Make columns equal height */
        .main-columns {
            display: flex;
            min-height: 700px;
        }

        /* Add some space between columns */
        .column-gap {
            padding: 0 10px;
        }

        /* Language toggle styling */
        .language-toggle {
            display: flex;
            justify-content: center;
            align-items: center;
            margin-bottom: 10px;
        }

        .language-button {
            margin: 0 5px;
            padding: 5px 10px;
            border-radius: 5px;
            cursor: pointer;
            font-weight: bold;
        }

        .active-lang {
            background-color: #4CAF50;
            color: white;
        }

        .inactive-lang {
            background-color: #f1f1f1;
            color: black;
        }
    </style>

    <script>
        // JavaScript ƒë·ªÉ t·ª± ƒë·ªông cu·ªôn xu·ªëng cu·ªëi ph·∫ßn chat
        function scrollToBottom() {
            var chatContainer = document.getElementById('chat-container');
            if (chatContainer) {
                chatContainer.scrollTop = chatContainer.scrollHeight;
            }
        }
        // Ch·∫°y h√†m khi trang ƒë√£ t·∫£i xong
        window.addEventListener('load', scrollToBottom);
    </script>
    """,
    unsafe_allow_html=True,
)

# Initialize language selection if not already set
if "language" not in st.session_state:
    st.session_state.language = "en"

# Check URL parameters for language change - PUT THIS BEFORE THE BUTTONS
query_params = st.query_params
if "lang" in query_params:
    if query_params["lang"] == "vn" and st.session_state.language != "vn":
        st.session_state.language = "vn"
        st.rerun()
    elif query_params["lang"] == "en" and st.session_state.language != "en":
        st.session_state.language = "en"
        st.rerun()


# Function to get text based on current language
def get_text(en_text, vn_text):
    return en_text if st.session_state.language == "en" else vn_text


# Call GPT
gemini_model = setup_gemini()

# Language toggle with buttons instead of HTML
col_spacer, lang_col = st.columns([10, 2])  # T·∫°o kh√¥ng gian r·ªông b√™n tr√°i, ch·ªâ s·ª≠ d·ª•ng 2/12 kh√¥ng gian cho n√∫t ng√¥n ng·ªØ

with lang_col:
    # T·∫°o 2 c·ªôt b√™n trong c·ªôt ng√¥n ng·ªØ ƒë·ªÉ ƒë·∫∑t c√°c n√∫t g·∫ßn nhau
    en_col, vn_col = st.columns(2)
    with en_col:
        if st.button("üá¨üáß EN", type="primary" if st.session_state.language == "en" else "secondary"):
            st.session_state.language = "en"
            st.query_params["lang"] = "en"
            st.rerun()
    with vn_col:
        if st.button("üáªüá≥ VN", type="primary" if st.session_state.language == "vn" else "secondary"):
            st.session_state.language = "vn"
            st.query_params["lang"] = "vn"
            st.rerun()

# Ti√™u ƒë·ªÅ ch√≠nh cho trang
st.markdown(
    f"<h1 style='text-align: center;'>üí¨ {get_text('Chatbot AI - Healthcare services', 'Chatbot AI - D·ªãch v·ª• chƒÉm s√≥c s·ª©c kh·ªèe')} üáªüá≥</h1>",
    unsafe_allow_html=True
)

# Create two main columns for the page layout
left_col, right_col = st.columns(2)

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []


# Function to get treatment recommendations based on current language
def get_treatment_recommendations(model, condition, lang="en"):
    if lang == "en":
        prompt = f"""
        Provide evidence-based treatment recommendations and care guidelines for a patient with {condition} diagnosis from an X-ray.

        Please format your response as follows:
        1. Brief explanation of the condition
        2. Standard treatment approaches
        3. Home care recommendations
        4. When to see a doctor
        5. Treatment details and the name of the medication that should be used


        Note: This is for informational purposes. Always consult healthcare professionals for proper medical advice.
        """
    else:  # Vietnamese
        prompt = f"""
        Cung c·∫•p c√°c khuy·∫øn ngh·ªã ƒëi·ªÅu tr·ªã d·ª±a tr√™n b·∫±ng ch·ª©ng v√† h∆∞·ªõng d·∫´n chƒÉm s√≥c cho b·ªánh nh√¢n c√≥ ch·∫©n ƒëo√°n {condition} t·ª´ h√¨nh ·∫£nh X-quang.

        Vui l√≤ng ƒë·ªãnh d·∫°ng ph·∫£n h·ªìi c·ªßa b·∫°n nh∆∞ sau:
        1. Gi·∫£i th√≠ch ng·∫Øn g·ªçn v·ªÅ t√¨nh tr·∫°ng
        2. Ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã ti√™u chu·∫©n
        3. Khuy·∫øn ngh·ªã chƒÉm s√≥c t·∫°i nh√†
        4. Khi n√†o n√™n g·∫∑p b√°c sƒ©
        5. Chi ti·∫øt v·ªÅ c√°ch ƒëi·ªÅu tr·ªã v√† c√°c t√™n c·ªßa lo·∫°i thu·ªëc n√™n s·ª≠ d·ª•ng
        
        L∆∞u √Ω: ƒê√¢y l√† th√¥ng tin tham kh·∫£o. Lu√¥n tham kh·∫£o √Ω ki·∫øn c·ªßa chuy√™n gia y t·∫ø ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n y t·∫ø th√≠ch h·ª£p.
        """

    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        error_msg = "Error in generating treatment recommendations" if lang == "en" else "L·ªói khi t·∫°o khuy·∫øn ngh·ªã ƒëi·ªÅu tr·ªã"
        return f"{error_msg}: {str(e)}"


# RIGHT COLUMN - X-ray Prediction
with right_col:
    st.subheader(get_text("üì∏ X-ray Prediction", "üì∏ D·ª± ƒëo√°n X-quang"))

    uploaded_file = st.file_uploader(
        get_text("Choose X-ray (JPG/PNG)", "Ch·ªçn ·∫£nh X-quang (JPG/PNG)"),
        type=["jpg", "png"]
    )

    if uploaded_file is not None:
        st.success(get_text("‚úÖ Image uploaded successfully!", "‚úÖ T·∫£i ·∫£nh l√™n th√†nh c√¥ng!"))

        # Add a toggle to show/hide the image
        show_image = st.checkbox(get_text("Show Image", "Hi·ªÉn th·ªã ·∫£nh"), value=True)

        # Only display the image if the toggle is on
        if show_image:
            st.image(uploaded_file, caption=" ", use_container_width=True)

        predict_button = st.button(get_text("Predict", "D·ª± ƒëo√°n"))

        if predict_button:
            try:
                # Image preprocessing
                image = Image.open(uploaded_file).convert("RGB")

                # L·∫•y thi·∫øt b·ªã hi·ªán t·∫°i
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

                # D·ª± ƒëo√°n k·∫øt qu·∫£
                model_lung = load_model()

                if model_lung is not None:
                    tensor = preprocess_image(image).to(device)
                    with torch.no_grad():
                        output = model_lung(tensor)
                        prediction = torch.argmax(output, 1).item()

                        classes = ['Covid', 'Normal', 'Pneumonia', 'Tuberculosis']
                        condition = classes[prediction]

                        result_prefix = "üßëüèΩ‚Äç‚öïÔ∏è üôèüèΩ **Prediction:**" if st.session_state.language == "en" else "üßëüèΩ‚Äç‚öïÔ∏è üôèüèΩ **K·∫øt qu·∫£ d·ª± ƒëo√°n:**"
                        result = f"{result_prefix} {condition}"
                        st.session_state.prediction_result = result
                        st.success(result)

                        # Get treatment recommendations for the condition
                        if condition != "Normal":
                            st.info(
                                get_text("Generating treatment recommendations...", "ƒêang t·∫°o khuy·∫øn ngh·ªã ƒëi·ªÅu tr·ªã..."))
                            treatment_info = get_treatment_recommendations(gemini_model, condition,
                                                                           st.session_state.language)
                            st.markdown(get_text("### Treatment Recommendations", "### Khuy·∫øn ngh·ªã ƒëi·ªÅu tr·ªã"))
                            st.markdown(treatment_info)
                        else:
                            st.markdown(get_text("### Normal X-ray", "### X-quang b√¨nh th∆∞·ªùng"))
                            normal_text = get_text(
                                "No specific treatment needed as the X-ray appears normal. Continue with regular health practices and consult with a healthcare provider for any persistent symptoms.",
                                "Kh√¥ng c·∫ßn ƒëi·ªÅu tr·ªã c·ª• th·ªÉ v√¨ X-quang c√≥ v·∫ª b√¨nh th∆∞·ªùng. Ti·∫øp t·ª•c th·ª±c h√†nh s·ª©c kh·ªèe th√¥ng th∆∞·ªùng v√† tham kh·∫£o √Ω ki·∫øn c·ªßa nh√† cung c·∫•p d·ªãch v·ª• chƒÉm s√≥c s·ª©c kh·ªèe n·∫øu c√≥ b·∫•t k·ª≥ tri·ªáu ch·ª©ng dai d·∫≥ng n√†o."
                            )
                            st.markdown(normal_text)

                else:
                    error_msg = "Cannot load model. Please, check your path." if st.session_state.language == "en" else "Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh. Vui l√≤ng ki·ªÉm tra ƒë∆∞·ªùng d·∫´n c·ªßa b·∫°n."
                    st.error(error_msg)
            except Exception as e:
                error_prefix = "Error:" if st.session_state.language == "en" else "L·ªói:"
                st.error(f"{error_prefix} {str(e)}")

# LEFT COLUMN - Chat Interface
with left_col:
    st.subheader(get_text("Chat with AI üñ•Ô∏è", "Tr√≤ chuy·ªán v·ªõi AI üñ•Ô∏è"))

    # Display chat history with ID for JavaScript
    st.markdown('<div id="chat-container">', unsafe_allow_html=True)
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
    st.markdown('</div>', unsafe_allow_html=True)

    # Form ƒë·ªÉ tr√°nh auto-rerun khi nh·∫≠p li·ªáu
    with st.form(key="chat_form", clear_on_submit=True):
        placeholder_text = get_text(
            "Clearly state your symptoms here, please üéÄ",
            "Vui l√≤ng m√¥ t·∫£ r√µ r√†ng c√°c tri·ªáu ch·ª©ng c·ªßa b·∫°n ·ªü ƒë√¢y üéÄ"
        )
        input_text = st.text_input("", placeholder=placeholder_text, key="user_input")
        submit_button = st.form_submit_button(get_text("Send", "G·ª≠i"))

    # X·ª≠ l√Ω khi c√≥ tin nh·∫Øn m·ªõi
    if submit_button and input_text.strip():
        # Th√™m tin nh·∫Øn ng∆∞·ªùi d√πng v√†o session state
        user_prefix = "You:" if st.session_state.language == "en" else "B·∫°n:"
        st.session_state.messages.append({"role": "user", "content": f"**{user_prefix}** {input_text}"})

        # Generate AI response
        thinking_text = "ü§ñ Thinking..." if st.session_state.language == "en" else "ü§ñ ƒêang suy nghƒ©..."
        with st.spinner(thinking_text):
            response = get_gemini_response(gemini_model, input_text, st.session_state.language)

        # Th√™m ph·∫£n h·ªìi c·ªßa AI v√†o session state
        ai_prefix = "AI:" if st.session_state.language == "en" else "AI:"
        st.session_state.messages.append({"role": "assistant", "content": f"**{ai_prefix}** {response}"})

        # L√†m m·ªõi trang m·ªôt l·∫ßn ƒë·ªÉ hi·ªÉn th·ªã tin nh·∫Øn m·ªõi
        st.rerun()
