
import streamlit as st
import os
import google.generativeai as genai
import torchvision.transforms as transforms
import torch
from PIL import Image
from model_lab1 import Cnn


# Gemini API setup
def setup_gemini():
    genai.configure(api_key='AIzaSyCLXHE3z4S8EzumKaIzBVZ0RGPL256oOmc')
    model = genai.GenerativeModel('gemini-pro')
    return model


# Disease prediction using Gemini
def get_gemini_response(model, symptoms):
    prompt = f"""
    Based on the following symptoms, suggest possible medical conditions and provide brief explanations:
    Symptoms: {symptoms}

    Please format your response as follows:
    1. Most likely conditions
    2. Brief explanation for each condition
    3. General recommendations

    Note: This is not a medical diagnosis. Please consult a healthcare professional for proper medical advice.
    """

    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Error in generating response: {str(e)}"


def preprocess_image(image_path):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),  # Resize v·ªÅ k√≠ch th∆∞·ªõc ph√π h·ª£p
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    return transform(image_path).unsqueeze(0)  # Th√™m chi·ªÅu cho ·∫£nh


def load_model():
    # S·ª≠ d·ª•ng thi·∫øt b·ªã ph√π h·ª£p (CPU ho·∫∑c GPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Kh·ªüi t·∫°o m√¥ h√¨nh Cnn v·ªõi 4 l·ªõp ph√¢n lo·∫°i
    model = Cnn(num_classes=4).to(device)

    # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file checkpoint
    checkpoint_path = os.path.join("trained_model", "best_cnn.pt")

    if not os.path.exists(checkpoint_path):
        st.error(f"Kh√¥ng t√¨m th·∫•y file model t·∫°i {checkpoint_path}")
        return None

    try:
        # C√°ch 1: Load v·ªõi weights_only=False (c√°ch an to√†n nh·∫•t n·∫øu b·∫°n tin t∆∞·ªüng ngu·ªìn checkpoint)
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint["model"])
    except Exception as e:
        try:
            # C√°ch 2: Th·ª≠ th√™m numpy.core.multiarray.scalar v√†o danh s√°ch safe globals
            import numpy as np
            from torch.serialization import add_safe_globals

            # Th√™m numpy.core.multiarray.scalar v√†o danh s√°ch safe globals
            add_safe_globals([np.core.multiarray.scalar])

            # Th·ª≠ load l·∫°i v·ªõi weights_only=True (m·∫∑c ƒë·ªãnh)
            checkpoint = torch.load(checkpoint_path, map_location=device)
            model.load_state_dict(checkpoint["model"])
        except Exception as inner_e:
            st.error(f"Kh√¥ng th·ªÉ load model: {str(inner_e)}")
            return None

    # Chuy·ªÉn sang ch·∫ø ƒë·ªô d·ª± ƒëo√°n
    model.eval()

    return model


# Streamlit page config
st.set_page_config(page_title="Chatbot AI", page_icon="ü§ñ", layout="wide")
st.markdown(
    """
    <style>
        .stChatMessage { border-radius: 10px; padding: 10px; margin: 5px 0; }
        .user { background-color: #dcf8c6; text-align: right; }
        .bot { background-color: #f1f1f1; }

        /* CSS ƒë·ªÉ t·ª± ƒë·ªông cu·ªôn xu·ªëng ph·∫ßn chat */
        #chat-container {
            max-height: 600px;
            overflow-y: auto;
        }

        /* X√≥a padding d∆∞·ªõi cho container */
        .block-container {
            padding-bottom: 2rem;
        }

        /* Make columns equal height */
        .main-columns {
            display: flex;
            min-height: 700px;
        }

        /* Add some space between columns */
        .column-gap {
            padding: 0 10px;
        }
    </style>

    <script>
        // JavaScript ƒë·ªÉ t·ª± ƒë·ªông cu·ªôn xu·ªëng cu·ªëi ph·∫ßn chat
        function scrollToBottom() {
            var chatContainer = document.getElementById('chat-container');
            if (chatContainer) {
                chatContainer.scrollTop = chatContainer.scrollHeight;
            }
        }
        // Ch·∫°y h√†m khi trang ƒë√£ t·∫£i xong
        window.addEventListener('load', scrollToBottom);
    </script>
    """,
    unsafe_allow_html=True,
)

# Call GPT
gemini_model = setup_gemini()

# Ti√™u ƒë·ªÅ ch√≠nh cho trang
st.markdown("<h1 style='text-align: center;'>üí¨ Chatbot AI - Healthcare services üáªüá≥</h1>", unsafe_allow_html=True)
# Create two main columns for the page layout
left_col, right_col = st.columns(2)

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# RIGHT COLUMN - X-ray Prediction
# Add this new function after your get_gemini_response function
def get_treatment_recommendations(model, condition):
    prompt = f"""
    Provide evidence-based treatment recommendations and care guidelines for a patient with {condition} diagnosis from an X-ray.

    Please format your response as follows:
    1. Brief explanation of the condition
    2. Standard treatment approaches
    3. Home care recommendations
    4. When to see a doctor

    Note: This is for informational purposes. Always consult healthcare professionals for proper medical advice.
    """

    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Error in generating treatment recommendations: {str(e)}"


# Then modify the prediction section in the right_col block to this:
# In the RIGHT COLUMN section, modify the code to include a toggle for the image

with right_col:
    st.subheader("üì∏ X-ray Prediction")

    uploaded_file = st.file_uploader("Choose X-ray (JPG/PNG)", type=["jpg", "png"])

    if uploaded_file is not None:
        st.success("‚úÖ Image uploaded successfully!")

        # Add a toggle to show/hide the image
        show_image = st.checkbox("Show Image", value=True)

        # Only display the image if the toggle is on
        if show_image:
            st.image(uploaded_file, caption=" ", use_container_width=True)

        predict_button = st.button("Predict")

        if predict_button:
            try:
                # Image preprocessing
                image = Image.open(uploaded_file).convert("RGB")

                # L·∫•y thi·∫øt b·ªã hi·ªán t·∫°i
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

                # D·ª± ƒëo√°n k·∫øt qu·∫£
                model_lung = load_model()

                if model_lung is not None:
                    tensor = preprocess_image(image).to(device)
                    with torch.no_grad():
                        output = model_lung(tensor)
                        prediction = torch.argmax(output, 1).item()

                        classes = ['Covid', 'Normal', 'Pneumonia', 'Tuberculosis']
                        condition = classes[prediction]
                        result = f"üßëüèΩ‚Äç‚öïÔ∏è üôèüèΩ **Prediction:** {condition}"
                        st.session_state.prediction_result = result
                        st.success(result)

                        # Get treatment recommendations for the condition
                        if condition != "Normal":
                            st.info("Generating treatment recommendations...")
                            treatment_info = get_treatment_recommendations(gemini_model, condition)
                            st.markdown("### Treatment Recommendations")
                            st.markdown(treatment_info)
                        else:
                            st.markdown("### Normal X-ray")
                            st.markdown(
                                "No specific treatment needed as the X-ray appears normal. Continue with regular health practices and consult with a healthcare provider for any persistent symptoms.")

                else:
                    st.error("Cannot load model. Please, check your path.")
            except Exception as e:
                st.error(f"Error: {str(e)}")

# LEFT COLUMN - Chat Interface
with left_col:
    st.subheader("Chat with AI üñ•Ô∏è ")

    # Display chat history with ID for JavaScript
    st.markdown('<div id="chat-container">', unsafe_allow_html=True)
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
    st.markdown('</div>', unsafe_allow_html=True)

    # Form ƒë·ªÉ tr√°nh auto-rerun khi nh·∫≠p li·ªáu
    with st.form(key="chat_form", clear_on_submit=True):

        input_text = st.text_input("", placeholder="Clearly state your symptoms here, please üéÄ", key="user_input")
        submit_button = st.form_submit_button("Send")

    # X·ª≠ l√Ω khi c√≥ tin nh·∫Øn m·ªõi
    if submit_button and input_text.strip():
        # Th√™m tin nh·∫Øn ng∆∞·ªùi d√πng v√†o session state
        st.session_state.messages.append({"role": "user", "content": f"**You:** {input_text}"})

        # Generate AI response
        with st.spinner("ü§ñ Thinking..."):
            response = get_gemini_response(gemini_model, input_text)

        # Th√™m ph·∫£n h·ªìi c·ªßa AI v√†o session state
        st.session_state.messages.append({"role": "assistant", "content": f"**AI:** {response}"})

        # L√†m m·ªõi trang m·ªôt l·∫ßn ƒë·ªÉ hi·ªÉn th·ªã tin nh·∫Øn m·ªõi
        st.rerun()